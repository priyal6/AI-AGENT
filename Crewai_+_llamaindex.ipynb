{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNV8sIe3sgkKYkVAuzx55P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyal6/AI-AGENT/blob/main/Crewai_%2B_llamaindex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0V8-sn8f4ok",
        "outputId": "33ca4464-41c9-4ad7-9c49-a8f952f6a6f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chromadb 0.5.23 requires tokenizers<=0.20.3,>=0.13.2, but you have tokenizers 0.21.1 which is incompatible.\n",
            "crewai 0.108.0 requires litellm==1.60.2, but you have litellm 1.63.14 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.49.0 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install llama-index --quiet\n",
        "!pip install llama-index-llms-groq --quiet\n",
        "!pip install llama-index-core --quiet\n",
        "!pip install llama-index-readers-file --quiet\n",
        "!pip install llama-index-tools-wolfram-alpha --quiet\n",
        "!pip install llama-index-embeddings-huggingface --quiet\n",
        "!pip install 'crewai[tools]' --quiet"
      ]
    },
    {
      "source": [
        "!pip install llama-index-llms-groq --quiet\n",
        "from llama_index.llms.groq import Groq"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bieBnUgg9vl",
        "outputId": "e85a2a00-0470-4e45-ce34-05a1840d67fd"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chromadb 0.5.23 requires tokenizers<=0.20.3,>=0.13.2, but you have tokenizers 0.21.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from litellm import completion\n",
        "import os\n",
        "\n",
        "os.environ['GROQ_API_KEY'] = \"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\"\n",
        "response = completion(\n",
        "    model=\"groq/llama3-8b-8192\",\n",
        "    messages=[\n",
        "       {\"role\": \"user\", \"content\": \"hello from litellm\"}\n",
        "   ],\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fut3ei-gF-bO",
        "outputId": "4f26fd2f-f8d5-4d3c-bbb7-5d70c806cef9"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModelResponse(id='chatcmpl-307b5048-9a52-4b64-841b-b6b2d3b2f272', created=1742852789, model='groq/llama3-8b-8192', object='chat.completion', system_fingerprint='fp_dadc9d6142', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Hello Litellm! It's nice to meet you. Is there something I can help you with, or would you like to chat about something in particular?\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=33, prompt_tokens=15, total_tokens=48, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.021187289, prompt_time=0.00241423, completion_time=0.0275, total_time=0.02991423), x_groq={'id': 'req_01jq51y5n7ek4rcbvy29tnea62'})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/httpx/_content.py:204: DeprecationWarning: Use 'content=<...>' to upload raw bytes/text content.\n",
            "  warnings.warn(message, DeprecationWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat based model for crew **ai**"
      ],
      "metadata": {
        "id": "6GEhexTaiKmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Define your API key here\n",
        "groq_api_key = \"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\"\n",
        "\n",
        "chat_llm = ChatOpenAI(\n",
        "    openai_api_base=\"https://api.groq.com/openai/v1\",\n",
        "    openai_api_key=groq_api_key,\n",
        "    model=\"llama3-8b-8192\",\n",
        "    temperature=0,\n",
        "    max_tokens=1000,\n",
        "    request_timeout=60\n",
        ")\n",
        "response = chat_llm.invoke(\"Hello, how are you?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "ZHcvfYBD1NiW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1dd52b8-668e-49dd-e921-dba7369bcfc3"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content=\"I'm just a language model, I don't have emotions or feelings like humans do, but I'm functioning properly and ready to help you with any questions or tasks you may have! How can I assist you today?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 45, 'prompt_tokens': 16, 'total_tokens': 61, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.019264971999999998, 'prompt_time': 0.002419618, 'completion_time': 0.0375, 'total_time': 0.039919618}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None} id='run-1ea37818-2ede-4a4f-935b-7f0b5c3c2df2-0' usage_metadata={'input_tokens': 16, 'output_tokens': 45, 'total_tokens': 61, 'input_token_details': {}, 'output_token_details': {}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://www.infosys.com/investors/reports-filings/annual-report/annual/documents/infosys-ar-23.pdf\" -O infy_nav.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQ-DXSGZj0gZ",
        "outputId": "02f39996-a841-464c-a4db-64e614582a36"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-24 21:46:30--  https://www.infosys.com/investors/reports-filings/annual-report/annual/documents/infosys-ar-23.pdf\n",
            "Resolving www.infosys.com (www.infosys.com)... 23.218.99.124\n",
            "Connecting to www.infosys.com (www.infosys.com)|23.218.99.124|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/pdf]\n",
            "Saving to: â€˜infy_nav.pdfâ€™\n",
            "\n",
            "infy_nav.pdf            [  <=>               ]  12.52M  43.6MB/s    in 0.3s    \n",
            "\n",
            "2025-03-24 21:46:30 (43.6 MB/s) - â€˜infy_nav.pdfâ€™ saved [13133646]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parsing the data"
      ],
      "metadata": {
        "id": "jFJ6pJC_kevL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
        "from llama_index.llms.openai import OpenAI\n",
        "import os\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "JjBSL_RYkimO"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reader = SimpleDirectoryReader(input_files=[\"infy_nav.pdf\"])\n",
        "docs = reader.load_data()\n",
        "docs[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcmeVfE7lb0k",
        "outputId": "85783090-7158-4085-d6ef-fbed1db4318e"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(id_='0f5057ec-45ff-44fd-a87b-9f1b0e491058', embedding=None, metadata={'page_label': '2', 'file_name': 'infy_nav.pdf', 'file_path': 'infy_nav.pdf', 'file_type': 'application/pdf', 'file_size': 13133646, 'creation_date': '2025-03-24', 'last_modified_date': '2023-06-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The context surrounding \\nan enterprise, created, \\nand influenced by multiple \\ninherently uncertain forces, can \\nsignificantly impact the fortunes \\nof a business. While this volatility \\nhas come to be expected as normal, \\nnot every business emerges from it having \\ntackled the situation with the same resilience. \\nThose that get a head start in preparing and taking \\non the challenges are also the ones that come out of the \\nuncertainty with the ability to outperform in the recovery.\\nThat does not necessarily mean rethinking oneâ€™s business \\nstrategy, but rather rearticulating it to adapt. Itâ€™s really \\nabout being judicious where to lean in and where to pull \\nback. In essence, itâ€™s about protecting oneâ€™s capability \\nto innovate and propel forward while being deeply \\ndisciplined when it comes to operations. Digitally \\ntransformed companies have a distinct advantage here. \\nThey can drive frictionless business throughout the supply \\nchain, serve customers at lower costs, and avoid resource-\\nintensive IT upgrades, unlike the digitally disadvantaged \\nwho may have to wait for economic conditions to improve \\nbefore they can make progress.\\nAdvanced technologies â€“ especially AI in tandem with \\ncloud â€“ are creating performance opportunities that are \\nreshaping these dynamics in significant ways. They are \\nhelping companies to amplify human potential to take \\non higher value work, boost people productivity and \\nrapidly create business value. From unlocking efficiencies \\nat scale and empowering the ecosystem to accelerating \\ngrowth, cognitively capable companies are unshackling \\nintelligence from data for experimentation as well as \\nthe scale-out of AI to power greater efficacy and create \\ndisruptive revenue streams. \\nInfosys is taking the AI-first approach to its own \\ntransformation. We are bringing the power of AI, analytics, \\nand cloud to accelerate our own enterprise transformation, \\neven as we build incremental value from micro-changes, \\nto improve client service, reimagine business processes, \\nand boost productivity. Nearly 50,000 reusable intelligent \\nservices, applied in over 25,000 instances, are amplifying \\nour employees to boost their productivity.\\nIt is Infosys TopazTM â€“ our AI-first services using generative \\nAI â€“ thatâ€™s making it all possible. Both for us and our clients. \\nToday, 12,000+ AI use cases and 150+ pre-trained AI \\nmodels delivered by AI-first specialists and data strategists \\nare accelerating business value for enterprises the world \\nover. All this, while ensuring uncompromising ethics, trust, \\nprivacy and compliance, and security of data and AI.\\nYouâ€™ll no doubt catch interesting glimpses of these exciting \\npossibilities and progress in this Integrated Annual Report. \\nItâ€™s the same potential we see too, to help our clients \\nnavigate change and move into a future filled with more \\npotential, and more shared advances.\\nNAVIGATING CHANGE AT THE PACE OF AI\\nInfosys Integrated Annual Report 2022-23 3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Model"
      ],
      "metadata": {
        "id": "C-nzhhjdqQwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
      ],
      "metadata": {
        "id": "wSJJSC9Prg4A"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
      ],
      "metadata": {
        "id": "cS8YVXkVrq96"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#index\n",
        "\n",
        "index = VectorStoreIndex.from_documents(docs, embed_model=embed_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNw-_jVDr5al",
        "outputId": "b54c17a7-2546-40e4-bc73-61eed65cda93"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
            "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query Engine"
      ],
      "metadata": {
        "id": "Y6vKdUPYtOOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine(similarity_top_k=5, llm=llm)"
      ],
      "metadata": {
        "id": "0wYhWHUxsEtP"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai_tools import LlamaIndexTool\n",
        "query_tool = LlamaIndexTool.from_query_engine(\n",
        "    query_engine,\n",
        "    name=\"infy Query Tool\",\n",
        "    description = \"Use this tool to lookup the 2022-23 Infosys Annual Report\"\n",
        ")"
      ],
      "metadata": {
        "id": "2ILr_Qo6tcPJ"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_tool.args_schema.schema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRdNmfKnuqmY",
        "outputId": "2b6a33af-2355-4c5f-86df-a991b55090b8"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-124-fe53991c08af>:1: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  query_tool.args_schema.schema()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'description': 'Schema for query tool.',\n",
              " 'properties': {'query': {'description': 'Search query for the query tool.',\n",
              "   'title': 'Query',\n",
              "   'type': 'string'}},\n",
              " 'required': ['query'],\n",
              " 'title': 'QueryToolSchema',\n",
              " 'type': 'object'}"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents"
      ],
      "metadata": {
        "id": "jekL9CuavwWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from crewai import Agent, Task, Crew, Process"
      ],
      "metadata": {
        "id": "O2dEUyruvv3_"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "researcher = Agent(\n",
        "    role = \"Lead Financial Analyst\",\n",
        "    goal= \"Uncover insights about different tech companies\",\n",
        "    backstory = \"\"\"You work at an asset management firm.\n",
        "  Your goal is to understand tech stocks like Infosys.\"\"\",\n",
        "    verbose=True,\n",
        "    allow_delegation = False,\n",
        "    tools = [query_tool],\n",
        "    llm = chat_llm\n",
        ")\n",
        "\n",
        "\n",
        "writer = Agent(\n",
        "    role = \"Tech Content Strategist\",\n",
        "    goal = \"Craft compelling content on tech advancements\",\n",
        "    backstory= \"\"\"You are a renowned Content Strategist, known for your insightful and engaging articles.\n",
        "  You transform complex concepts into compelling narratives.\"\"\",\n",
        "    llm= chat_llm,\n",
        "    verbose=True,\n",
        "    allow_delegation=False,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3R40xYBv6dB",
        "outputId": "3e4c867d-881c-4f8f-8e64-16ef089b0115"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tasks"
      ],
      "metadata": {
        "id": "T6TtR3m_q9sE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task1 = Task(\n",
        "    description = \"\"\"Conduct a comprehensive analysis of Infosys's use of artificial intelligence in their integrated annual report 2022-23.\"\"\",\n",
        "    expected_output = \"Full analysis report in bullet points\",\n",
        "    agent = researcher,\n",
        ")\n",
        "\n",
        "task2 = Task(\n",
        "    description = \"\"\"Using the insights provided, develop an engaging blog\n",
        "  post that highlights the importance of artificial intelligence at Infosys.\n",
        "  Your post should be informative yet accessible, catering to a casual audience.\n",
        "  Make it sound cool, avoid complex words.\"\"\",\n",
        "    expected_output = \"Full blog post of atleast 4 paragraphs\",\n",
        "    agent = writer,\n",
        ")"
      ],
      "metadata": {
        "id": "682y6ou5q7uQ"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# crew"
      ],
      "metadata": {
        "id": "TBnJr_1kr4In"
      }
    },
    {
      "source": [
        "crew = Crew(\n",
        "    agents = [researcher, writer],\n",
        "    tasks = [task1, task2],\n",
        "    verbose = True, # Changed verbose to True for detailed output, or False for minimal output\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_k75jMEsQEn",
        "outputId": "e69efaee-53c3-44c9-922c-4f025f689cf3"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install crewai --upgrade --quiet"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nNUlgFZtG3F",
        "outputId": "8902010b-653f-406c-fbf6-d26dc8b3c963"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.49.0 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install crewai crewai[tools] --upgrade --quiet"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "7Oor8JN-tov3"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "source": [
        "import time\n",
        "   # ... your existing code ...\n",
        "\n",
        "   # Wait for the suggested time before retrying\n",
        "time.sleep(4.879)\n",
        "\n",
        "   # Retry the crew.kickoff() command\n",
        "result = crew.kickoff()\n",
        "print(\"######################\")\n",
        "print(result)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mcvWn1fLJk0y",
        "outputId": "aec83d63-9a4f-46b6-ef19-e80e2ed5d6f5"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[36mâ•­â”€\u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36m Crew Execution Started \u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36mâ”€â•®\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m                                                                                                                 \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m  \u001b[1;36mCrew Execution Started\u001b[0m                                                                                         \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m  \u001b[37mName: \u001b[0m\u001b[36mcrew\u001b[0m                                                                                                     \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m  \u001b[37mID: \u001b[0m\u001b[36mbae36851-f743-47a8-9022-22b57cdae958\u001b[0m                                                                       \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m                                                                                                                 \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m                                                                                                                 \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Crew Execution Started â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Crew Execution Started</span>                                                                                         <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Name: </span><span style=\"color: #008080; text-decoration-color: #008080\">crew</span>                                                                                                     <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">ID: </span><span style=\"color: #008080; text-decoration-color: #008080\">bae36851-f743-47a8-9022-22b57cdae958</span>                                                                       <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36mğŸš€ Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
              "â””â”€â”€ \u001b[1;33mğŸ“‹ Task: d285ea39-63c8-4c6c-b1fa-71db07963a9b\u001b[0m\n",
              "    \u001b[37m   Status: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ğŸš€ Crew: crew</span>\n",
              "â””â”€â”€ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ğŸ“‹ Task: d285ea39-63c8-4c6c-b1fa-71db07963a9b</span>\n",
              "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36mğŸš€ Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
              "â””â”€â”€ \u001b[1;33mğŸ“‹ Task: d285ea39-63c8-4c6c-b1fa-71db07963a9b\u001b[0m\n",
              "    \u001b[37m   Status: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n",
              "    â””â”€â”€ \u001b[1;32mğŸ¤– Agent: \u001b[0m\u001b[32mLead Financial Analyst\u001b[0m\n",
              "        \u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ğŸš€ Crew: crew</span>\n",
              "â””â”€â”€ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ğŸ“‹ Task: d285ea39-63c8-4c6c-b1fa-71db07963a9b</span>\n",
              "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n",
              "    â””â”€â”€ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">ğŸ¤– Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Lead Financial Analyst</span>\n",
              "        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mLead Financial Analyst\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mConduct a comprehensive analysis of Infosys's use of artificial intelligence in their integrated annual report 2022-23.\u001b[00m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36mğŸš€ Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
              "â””â”€â”€ \u001b[1;33mğŸ“‹ Task: d285ea39-63c8-4c6c-b1fa-71db07963a9b\u001b[0m\n",
              "    \u001b[37m   Status: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n",
              "    â””â”€â”€ \u001b[1;32mğŸ¤– Agent: \u001b[0m\u001b[32mLead Financial Analyst\u001b[0m\n",
              "        \u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n",
              "        â””â”€â”€ \u001b[1;34mğŸ§  \u001b[0m\u001b[34mThinking...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ğŸš€ Crew: crew</span>\n",
              "â””â”€â”€ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ğŸ“‹ Task: d285ea39-63c8-4c6c-b1fa-71db07963a9b</span>\n",
              "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n",
              "    â””â”€â”€ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">ğŸ¤– Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Lead Financial Analyst</span>\n",
              "        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
              "        â””â”€â”€ <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">ğŸ§  </span><span style=\"color: #000080; text-decoration-color: #000080\">Thinking...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36mğŸš€ Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
              "â””â”€â”€ \u001b[1;33mğŸ“‹ Task: d285ea39-63c8-4c6c-b1fa-71db07963a9b\u001b[0m\n",
              "    \u001b[37m   Status: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n",
              "    â””â”€â”€ \u001b[1;32mğŸ¤– Agent: \u001b[0m\u001b[32mLead Financial Analyst\u001b[0m\n",
              "        \u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n",
              "        â””â”€â”€ \u001b[1;31mâŒ LLM Failed\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ğŸš€ Crew: crew</span>\n",
              "â””â”€â”€ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ğŸ“‹ Task: d285ea39-63c8-4c6c-b1fa-71db07963a9b</span>\n",
              "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n",
              "    â””â”€â”€ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">ğŸ¤– Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Lead Financial Analyst</span>\n",
              "        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
              "        â””â”€â”€ <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">âŒ LLM Failed</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31mâ•­â”€\u001b[0m\u001b[31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[31m LLM Error \u001b[0m\u001b[31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[31mâ”€â•®\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m                                                                                                                 \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m  \u001b[1;31mâŒ LLM Call Failed\u001b[0m                                                                                             \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m  \u001b[37mError: \u001b[0m\u001b[31mlitellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. \u001b[0m   \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m  \u001b[31mYou passed model=llama3-8b-8192\u001b[0m                                                                                \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m  \u001b[31m Pass model as E.g. For 'Huggingface' inference endpoints pass in \u001b[0m                                             \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m  \u001b[31m`completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\u001b[0m              \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m                                                                                                                 \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LLM Error â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>  <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">âŒ LLM Call Failed</span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Error: </span><span style=\"color: #800000; text-decoration-color: #800000\">litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. </span>   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>  <span style=\"color: #800000; text-decoration-color: #800000\">You passed model=llama3-8b-8192</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>  <span style=\"color: #800000; text-decoration-color: #800000\"> Pass model as E.g. For 'Huggingface' inference endpoints pass in </span>                                             <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>  <span style=\"color: #800000; text-decoration-color: #800000\">`completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers</span>              <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:LiteLLM call failed: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llama3-8b-8192\n",
            " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[91m Error during LLM call: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llama3-8b-8192\n",
            " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\u001b[00m\n",
            "\u001b[91m An unknown error occurred. Please check the details below.\u001b[00m\n",
            "\u001b[91m Error details: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llama3-8b-8192\n",
            " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\u001b[00m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36mğŸš€ Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
              "â””â”€â”€ \u001b[1;31mğŸ“‹ Task: d285ea39-63c8-4c6c-b1fa-71db07963a9b\u001b[0m\n",
              "    \u001b[37m   Assigned to: \u001b[0m\u001b[31mLead Financial Analyst\u001b[0m\n",
              "    \u001b[37m   Status: \u001b[0m\u001b[1;31mâŒ Failed\u001b[0m\n",
              "    â””â”€â”€ \u001b[1;32mğŸ¤– Agent: \u001b[0m\u001b[32mLead Financial Analyst\u001b[0m\n",
              "        \u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n",
              "        â””â”€â”€ \u001b[1;31mâŒ LLM Failed\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ğŸš€ Crew: crew</span>\n",
              "â””â”€â”€ <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">ğŸ“‹ Task: d285ea39-63c8-4c6c-b1fa-71db07963a9b</span>\n",
              "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #800000; text-decoration-color: #800000\">Lead Financial Analyst</span>\n",
              "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">âŒ Failed</span>\n",
              "    â””â”€â”€ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">ğŸ¤– Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Lead Financial Analyst</span>\n",
              "        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
              "        â””â”€â”€ <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">âŒ LLM Failed</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31mâ•­â”€\u001b[0m\u001b[31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[31m Task Failure \u001b[0m\u001b[31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[31mâ”€â•®\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m                                                                                                                 \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m  \u001b[1;31mTask Failed\u001b[0m                                                                                                    \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m  \u001b[37mName: \u001b[0m\u001b[31md285ea39-63c8-4c6c-b1fa-71db07963a9b\u001b[0m                                                                     \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m  \u001b[37mAgent: \u001b[0m\u001b[31mLead Financial Analyst\u001b[0m                                                                                  \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m                                                                                                                 \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m                                                                                                                 \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Task Failure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>  <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Task Failed</span>                                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Name: </span><span style=\"color: #800000; text-decoration-color: #800000\">d285ea39-63c8-4c6c-b1fa-71db07963a9b</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Agent: </span><span style=\"color: #800000; text-decoration-color: #800000\">Lead Financial Analyst</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31mâ•­â”€\u001b[0m\u001b[31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[31m Crew Failure \u001b[0m\u001b[31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[31mâ”€â•®\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m                                                                                                                 \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m  \u001b[1;31mCrew Execution Failed\u001b[0m                                                                                          \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m  \u001b[37mName: \u001b[0m\u001b[31mcrew\u001b[0m                                                                                                     \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m  \u001b[37mID: \u001b[0m\u001b[31mbae36851-f743-47a8-9022-22b57cdae958\u001b[0m                                                                       \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m                                                                                                                 \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m                                                                                                                 \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Crew Failure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>  <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Crew Execution Failed</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Name: </span><span style=\"color: #800000; text-decoration-color: #800000\">crew</span>                                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">ID: </span><span style=\"color: #800000; text-decoration-color: #800000\">bae36851-f743-47a8-9022-22b57cdae958</span>                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llama3-8b-8192\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-129-7d6b28760404>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m    \u001b[0;31m# Retry the crew.kickoff() command\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkickoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"######################\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crewai/crew.py\u001b[0m in \u001b[0;36mkickoff\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sequential_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhierarchical\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_hierarchical_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crewai/crew.py\u001b[0m in \u001b[0;36m_run_sequential_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_sequential_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mCrewOutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;34m\"\"\"Executes tasks sequentially and returns the final output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_tasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_hierarchical_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mCrewOutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crewai/crew.py\u001b[0m in \u001b[0;36m_execute_tasks\u001b[0;34m(self, tasks, start_index, was_replayed)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m                 task_output = task.execute_sync(\n\u001b[0m\u001b[1;32m    851\u001b[0m                     \u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent_to_use\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m                     \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crewai/task.py\u001b[0m in \u001b[0;36mexecute_sync\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    308\u001b[0m     ) -> TaskOutput:\n\u001b[1;32m    309\u001b[0m         \u001b[0;34m\"\"\"Execute the task synchronously.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crewai/task.py\u001b[0m in \u001b[0;36m_execute_core\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mcrewai_event_bus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTaskFailedEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m  \u001b[0;31m# Re-raise the exception after emitting the event\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crewai/task.py\u001b[0m in \u001b[0;36m_execute_core\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_by_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0mcrewai_event_bus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTaskStartedEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             result = agent.execute_task(\n\u001b[0m\u001b[1;32m    375\u001b[0m                 \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crewai/agent.py\u001b[0m in \u001b[0;36mexecute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    264\u001b[0m                     ),\n\u001b[1;32m    265\u001b[0m                 )\n\u001b[0;32m--> 266\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_times_executed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_times_executed\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retry_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crewai/agent.py\u001b[0m in \u001b[0;36mexecute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    245\u001b[0m                 ),\n\u001b[1;32m    246\u001b[0m             )\n\u001b[0;32m--> 247\u001b[0;31m             result = self.agent_executor.invoke(\n\u001b[0m\u001b[1;32m    248\u001b[0m                 {\n\u001b[1;32m    249\u001b[0m                     \u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtask_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crewai/agents/crew_agent_executor.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"litellm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;31m# Do not retry on litellm errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crewai/agents/crew_agent_executor.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mformatted_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             self._printer.print(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crewai/agents/crew_agent_executor.py\u001b[0m in \u001b[0;36m_invoke_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"litellm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                     \u001b[0;31m# Do not retry on litellm errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_context_length_exceeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_context_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crewai/agents/crew_agent_executor.py\u001b[0m in \u001b[0;36m_invoke_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enforce_rpm_limit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_llm_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m                 \u001b[0mformatted_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_llm_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crewai/agents/crew_agent_executor.py\u001b[0m in \u001b[0;36m_get_llm_response\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"red\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             )\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crewai/agents/crew_agent_executor.py\u001b[0m in \u001b[0;36m_get_llm_response\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;34m\"\"\"Call the LLM and return the response, handling any invalid responses.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             answer = self.llm.call(\n\u001b[0m\u001b[1;32m    208\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crewai/llm.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, messages, tools, callbacks, available_functions)\u001b[0m\n\u001b[1;32m    737\u001b[0m                     )\n\u001b[1;32m    738\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m                     return self._handle_non_streaming_response(\n\u001b[0m\u001b[1;32m    740\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavailable_functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crewai/llm.py\u001b[0m in \u001b[0;36m_handle_non_streaming_response\u001b[0;34m(self, params, callbacks, available_functions)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \"\"\"\n\u001b[1;32m    574\u001b[0m         \u001b[0;31m# --- 1) Make the completion call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlitellm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompletion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;31m# --- 2) Extract response message and content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m                     \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback_exception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m                 )  # DO NOT MAKE THREADED - router retry fallback relies on this!\n\u001b[0;32m-> 1154\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1030\u001b[0m                     \u001b[0mprint_verbose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error while checking max token limit: {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m             \u001b[0;31m# MODEL CALL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"stream\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/main.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   3066\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m         \u001b[0;31m## Map to OpenAI Exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3068\u001b[0;31m         raise exception_type(\n\u001b[0m\u001b[1;32m   3069\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m             \u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/main.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeployment_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m             \u001b[0mcustom_llm_provider\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"azure\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m         model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m             \u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\u001b[0m in \u001b[0;36mget_llm_provider\u001b[0;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitellm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBadRequestError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             error_str = (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\u001b[0m in \u001b[0;36mget_llm_provider\u001b[0;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0merror_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model={model}\\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;31m# maps to openai.NotFoundError, this is raised when openai does not recognize the llm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             raise litellm.exceptions.BadRequestError(  # type: ignore\n\u001b[0m\u001b[1;32m    334\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequestError\u001b[0m: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llama3-8b-8192\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers"
          ]
        }
      ]
    }
  ]
}